#!/usr/bin/env python3
"""
Super Prompt Engineer - Context Engineering Enhanced
Creates comprehensive, detailed prompts from single lines of input
"""

import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

class PromptType(Enum):
    """Types of prompts the engineer can create"""
    DESIGN_CHALLENGE = "design_challenge"
    TECHNICAL_SPECIFICATION = "technical_specification"
    USER_RESEARCH = "user_research"
    STRATEGY_DEVELOPMENT = "strategy_development"
    CREATIVE_BRIEF = "creative_brief"
    SYSTEM_ARCHITECTURE = "system_architecture"
    BUSINESS_ANALYSIS = "business_analysis"

class ContextDimension(Enum):
    """Context dimensions for prompt enhancement"""
    DOMAIN_EXPERTISE = "domain_expertise"
    USER_PERSPECTIVE = "user_perspective"
    BUSINESS_CONTEXT = "business_context"
    TECHNICAL_CONSTRAINTS = "technical_constraints"
    REGULATORY_ENVIRONMENT = "regulatory_environment"
    COMPETITIVE_LANDSCAPE = "competitive_landscape"
    TEMPORAL_CONTEXT = "temporal_context"

@dataclass
class PromptEnhancement:
    """Structure for prompt enhancement data"""
    context_layers: Dict[str, Any]
    specificity_level: float
    complexity_score: float
    actionability_score: float
    completeness_rating: float

class SuperPromptEngineer:
    """
    Context Engineering Super Prompt Engineer
    Transforms single lines into comprehensive, detailed prompts
    """
    
    def __init__(self):
        self.context_templates = self._build_context_templates()
        self.domain_knowledge = self._build_domain_knowledge()
        self.prompt_patterns = self._build_prompt_patterns()
        self.enhancement_strategies = self._build_enhancement_strategies()
        
    def _build_context_templates(self) -> Dict[str, Dict[str, Any]]:
        """Build comprehensive context templates for different domains"""
        return {
            "fintech": {
                "regulatory_context": {
                    "compliance_requirements": ["KYC", "AML", "SOX", "GDPR", "PCI-DSS"],
                    "regulatory_bodies": ["SEC", "FINRA", "CFTC", "OCC", "FDIC"],
                    "international_standards": ["Basel III", "MiFID II", "PSD2"]
                },
                "technical_context": {
                    "security_standards": ["Zero Trust", "Defense in Depth", "Continuous Monitoring"],
                    "scalability_requirements": ["High Availability", "Disaster Recovery", "Global Scale"],
                    "integration_patterns": ["API-First", "Microservices", "Event-Driven"]
                },
                "user_context": {
                    "user_types": ["Retail Investors", "Institutional Traders", "Crypto Enthusiasts"],
                    "experience_levels": ["Novice", "Intermediate", "Expert"],
                    "trust_factors": ["Security", "Transparency", "Compliance", "Performance"]
                }
            },
            "healthcare": {
                "regulatory_context": {
                    "compliance_requirements": ["HIPAA", "HITECH", "FDA", "SOX"],
                    "quality_standards": ["ISO 27001", "SOC 2", "HITRUST"],
                    "clinical_standards": ["HL7", "FHIR", "ICD-10", "CPT"]
                },
                "technical_context": {
                    "interoperability": ["EHR Integration", "API Standards", "Data Exchange"],
                    "security_requirements": ["PHI Protection", "Access Controls", "Audit Trails"],
                    "reliability_standards": ["99.99% Uptime", "Disaster Recovery", "Data Backup"]
                },
                "user_context": {
                    "stakeholders": ["Patients", "Providers", "Administrators", "Regulators"],
                    "workflows": ["Clinical", "Administrative", "Billing", "Research"],
                    "accessibility": ["ADA Compliance", "Multi-language", "Low Literacy"]
                }
            },
            "ecommerce": {
                "business_context": {
                    "revenue_models": ["B2C", "B2B", "Marketplace", "Subscription"],
                    "growth_metrics": ["CAC", "LTV", "Conversion Rate", "AOV"],
                    "competitive_factors": ["Price", "Selection", "Speed", "Experience"]
                },
                "technical_context": {
                    "performance_requirements": ["Sub-second Load", "Global CDN", "Mobile-First"],
                    "scalability_needs": ["Peak Traffic", "Inventory Management", "Payment Processing"],
                    "integration_requirements": ["Payment Gateways", "Logistics", "Analytics"]
                },
                "user_context": {
                    "customer_journey": ["Discovery", "Consideration", "Purchase", "Support"],
                    "device_usage": ["Mobile", "Desktop", "Tablet", "Voice"],
                    "trust_signals": ["Reviews", "Security", "Return Policy", "Support"]
                }
            }
        }
    
    def _build_domain_knowledge(self) -> Dict[str, Dict[str, Any]]:
        """Build comprehensive domain knowledge base"""
        return {
            "design_patterns": {
                "authentication": {
                    "patterns": ["Progressive Authentication", "Risk-Based Authentication", "Biometric Authentication"],
                    "considerations": ["Security vs Usability", "Compliance Requirements", "User Trust"],
                    "best_practices": ["MFA by Default", "Graceful Degradation", "Clear Communication"]
                },
                "onboarding": {
                    "patterns": ["Progressive Onboarding", "Contextual Onboarding", "Social Onboarding"],
                    "considerations": ["Time to Value", "Cognitive Load", "Drop-off Rates"],
                    "best_practices": ["Show Progress", "Immediate Value", "Optional Steps"]
                },
                "trust_building": {
                    "patterns": ["Social Proof", "Authority Indicators", "Transparency Signals"],
                    "considerations": ["User Psychology", "Cultural Differences", "Risk Perception"],
                    "best_practices": ["Clear Communication", "Consistent Experience", "Proactive Support"]
                }
            },
            "technical_patterns": {
                "scalability": {
                    "patterns": ["Microservices", "Event-Driven Architecture", "CQRS"],
                    "considerations": ["Performance", "Complexity", "Team Structure"],
                    "best_practices": ["Start Monolith", "Domain Boundaries", "Async Communication"]
                },
                "security": {
                    "patterns": ["Zero Trust", "Defense in Depth", "Least Privilege"],
                    "considerations": ["Threat Model", "Compliance", "User Experience"],
                    "best_practices": ["Security by Design", "Regular Audits", "Incident Response"]
                }
            }
        }
    
    def _build_prompt_patterns(self) -> Dict[str, Dict[str, Any]]:
        """Build prompt enhancement patterns"""
        return {
            "structure_patterns": {
                "design_challenge": {
                    "opening": "Design a {specificity} {domain} solution that {primary_goal}",
                    "constraints": "while addressing {constraints}",
                    "context": "Consider the {context_factors}",
                    "outcomes": "Deliver {expected_outcomes}",
                    "success_criteria": "Success measured by {success_metrics}"
                },
                "technical_specification": {
                    "opening": "Architect a {complexity} {technology} system for {use_case}",
                    "requirements": "that meets {performance_requirements}",
                    "constraints": "within {technical_constraints}",
                    "integration": "integrating with {existing_systems}",
                    "standards": "following {industry_standards}"
                }
            },
            "enhancement_patterns": {
                "specificity_enhancers": [
                    "Define specific user personas and their needs",
                    "Identify measurable success criteria",
                    "Specify technical requirements and constraints",
                    "Detail regulatory and compliance considerations",
                    "Outline competitive differentiation factors"
                ],
                "context_enhancers": [
                    "Industry-specific regulations and standards",
                    "Current market conditions and trends",
                    "Technological capabilities and limitations",
                    "User behavior patterns and preferences",
                    "Business model and revenue considerations"
                ]
            }
        }
    
    def _build_enhancement_strategies(self) -> Dict[str, List[str]]:
        """Build strategies for different types of enhancements"""
        return {
            "context_expansion": [
                "Add relevant industry context and regulations",
                "Include user persona and behavior patterns",
                "Specify technical constraints and requirements",
                "Detail business objectives and success metrics",
                "Incorporate competitive landscape considerations"
            ],
            "specificity_enhancement": [
                "Replace general terms with specific, measurable criteria",
                "Add quantitative targets and success metrics",
                "Specify technologies, platforms, and tools",
                "Define clear scope and boundaries",
                "Include timeline and resource constraints"
            ],
            "actionability_improvement": [
                "Break down into specific, executable tasks",
                "Define clear deliverables and outcomes",
                "Specify roles and responsibilities",
                "Include validation and testing criteria",
                "Add iteration and feedback mechanisms"
            ]
        }
    
    def engineer_super_prompt(self, input_line: str) -> Dict[str, Any]:
        """
        Transform a single line into a comprehensive, detailed prompt
        
        Args:
            input_line: Single line of input from user
            
        Returns:
            Comprehensive prompt with context, specificity, and actionability
        """
        
        print(f"üöÄ SUPER PROMPT ENGINEERING")
        print(f"üìù Input: {input_line}")
        print()
        
        # Phase 1: Analyze input
        analysis = self._analyze_input(input_line)
        
        # Phase 2: Gather context
        context = self._gather_comprehensive_context(analysis)
        
        # Phase 3: Enhance specificity
        enhanced_prompt = self._enhance_specificity(input_line, analysis, context)
        
        # Phase 4: Add actionability
        actionable_prompt = self._add_actionability(enhanced_prompt, analysis, context)
        
        # Phase 5: Validate and score
        validation = self._validate_prompt_quality(actionable_prompt)
        
        return {
            "original_input": input_line,
            "analysis": analysis,
            "context": context,
            "enhanced_prompt": actionable_prompt,
            "validation": validation,
            "enhancement_metrics": {
                "specificity_improvement": validation["specificity_score"],
                "context_richness": validation["context_score"],
                "actionability": validation["actionability_score"],
                "overall_quality": validation["overall_score"]
            }
        }
    
    def _analyze_input(self, input_line: str) -> Dict[str, Any]:
        """Analyze the input to understand intent, domain, and complexity"""
        
        # Extract key elements
        keywords = input_line.lower().split()
        
        # Determine domain
        domain = self._detect_domain(keywords)
        
        # Determine prompt type
        prompt_type = self._detect_prompt_type(keywords)
        
        # Assess complexity level
        complexity = self._assess_complexity(input_line)
        
        # Extract primary goal
        primary_goal = self._extract_primary_goal(input_line)
        
        # Identify missing context
        missing_context = self._identify_missing_context(input_line, domain)
        
        return {
            "domain": domain,
            "prompt_type": prompt_type,
            "complexity_level": complexity,
            "primary_goal": primary_goal,
            "missing_context": missing_context,
            "keywords": keywords,
            "input_length": len(input_line.split()),
            "specificity_score": min(0.4, len(keywords) * 0.05)  # Initially low
        }
    
    def _detect_domain(self, keywords: List[str]) -> str:
        """Detect the domain from keywords"""
        domain_indicators = {
            "fintech": ["crypto", "trading", "financial", "payment", "banking", "compliance", "regulatory"],
            "healthcare": ["medical", "patient", "clinical", "healthcare", "hipaa", "ehr"],
            "ecommerce": ["shopping", "cart", "checkout", "product", "inventory", "marketplace"],
            "saas": ["subscription", "dashboard", "analytics", "reporting", "user management"],
            "mobile": ["app", "mobile", "ios", "android", "responsive"],
            "ai": ["machine learning", "artificial intelligence", "nlp", "computer vision"],
            "security": ["authentication", "authorization", "security", "privacy", "encryption"]
        }
        
        for domain, indicators in domain_indicators.items():
            if any(indicator in " ".join(keywords) for indicator in indicators):
                return domain
        
        return "general"
    
    def _detect_prompt_type(self, keywords: List[str]) -> PromptType:
        """Detect the type of prompt needed"""
        type_indicators = {
            PromptType.DESIGN_CHALLENGE: ["design", "create", "build", "develop"],
            PromptType.TECHNICAL_SPECIFICATION: ["architect", "implement", "technical", "system"],
            PromptType.USER_RESEARCH: ["research", "understand", "analyze", "investigate"],
            PromptType.STRATEGY_DEVELOPMENT: ["strategy", "plan", "roadmap", "approach"],
            PromptType.CREATIVE_BRIEF: ["creative", "campaign", "content", "messaging"],
            PromptType.BUSINESS_ANALYSIS: ["business", "market", "competitive", "revenue"]
        }
        
        for prompt_type, indicators in type_indicators.items():
            if any(indicator in keywords for indicator in indicators):
                return prompt_type
        
        return PromptType.DESIGN_CHALLENGE
    
    def _assess_complexity(self, input_line: str) -> str:
        """Assess the complexity level of the challenge"""
        complexity_indicators = {
            "simple": ["basic", "simple", "easy", "quick"],
            "moderate": ["standard", "typical", "normal"],
            "complex": ["complex", "advanced", "sophisticated", "enterprise"],
            "expert": ["cutting-edge", "revolutionary", "breakthrough", "innovative"]
        }
        
        input_lower = input_line.lower()
        
        for level, indicators in complexity_indicators.items():
            if any(indicator in input_lower for indicator in indicators):
                return level
        
        # Default based on length and domain
        if len(input_line.split()) > 10:
            return "complex"
        elif len(input_line.split()) > 5:
            return "moderate"
        else:
            return "simple"
    
    def _extract_primary_goal(self, input_line: str) -> str:
        """Extract the primary goal from the input"""
        goal_patterns = {
            "trust": ["trust", "confidence", "reliability", "credibility"],
            "security": ["secure", "protection", "safety", "privacy"],
            "usability": ["easy", "simple", "intuitive", "user-friendly"],
            "performance": ["fast", "efficient", "scalable", "optimized"],
            "compliance": ["compliant", "regulatory", "standards", "audit"],
            "innovation": ["innovative", "breakthrough", "cutting-edge", "revolutionary"]
        }
        
        input_lower = input_line.lower()
        
        for goal, patterns in goal_patterns.items():
            if any(pattern in input_lower for pattern in patterns):
                return goal
        
        return "effectiveness"
    
    def _identify_missing_context(self, input_line: str, domain: str) -> List[str]:
        """Identify what context is missing from the input"""
        
        missing_context = []
        
        # Check for user context
        if not any(word in input_line.lower() for word in ["user", "customer", "persona", "audience"]):
            missing_context.append("user_personas")
        
        # Check for business context
        if not any(word in input_line.lower() for word in ["business", "revenue", "cost", "value"]):
            missing_context.append("business_objectives")
        
        # Check for technical context
        if not any(word in input_line.lower() for word in ["platform", "technology", "integration", "api"]):
            missing_context.append("technical_requirements")
        
        # Check for regulatory context (especially for fintech)
        if domain == "fintech" and not any(word in input_line.lower() for word in ["compliance", "regulatory", "audit"]):
            missing_context.append("regulatory_compliance")
        
        # Check for competitive context
        if not any(word in input_line.lower() for word in ["competitive", "market", "differentiation"]):
            missing_context.append("competitive_landscape")
        
        return missing_context
    
    def _gather_comprehensive_context(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Gather comprehensive context based on analysis"""
        
        domain = analysis["domain"]
        missing_context = analysis["missing_context"]
        
        context = {}
        
        # Add domain-specific context
        if domain in self.context_templates:
            context.update(self.context_templates[domain])
        
        # Add missing context elements
        for missing in missing_context:
            if missing == "user_personas":
                context["user_personas"] = self._generate_user_personas(domain)
            elif missing == "business_objectives":
                context["business_objectives"] = self._generate_business_objectives(domain)
            elif missing == "technical_requirements":
                context["technical_requirements"] = self._generate_technical_requirements(domain)
            elif missing == "regulatory_compliance":
                context["regulatory_compliance"] = self._generate_regulatory_context(domain)
            elif missing == "competitive_landscape":
                context["competitive_landscape"] = self._generate_competitive_context(domain)
        
        return context
    
    def _generate_user_personas(self, domain: str) -> Dict[str, Any]:
        """Generate relevant user personas for the domain"""
        personas = {
            "fintech": {
                "primary": "Tech-savvy millennial investor with moderate crypto experience",
                "secondary": "Traditional investor new to digital assets",
                "tertiary": "Institutional trader requiring advanced features"
            },
            "healthcare": {
                "primary": "Healthcare provider focused on patient care efficiency",
                "secondary": "Patient seeking convenient access to health information",
                "tertiary": "Healthcare administrator managing compliance"
            },
            "ecommerce": {
                "primary": "Mobile-first shopper seeking convenience and value",
                "secondary": "Research-driven buyer comparing options",
                "tertiary": "Repeat customer expecting personalized experience"
            }
        }
        
        return personas.get(domain, {
            "primary": "Primary user seeking efficient solution",
            "secondary": "Secondary user with specific needs",
            "tertiary": "Power user requiring advanced capabilities"
        })
    
    def _generate_business_objectives(self, domain: str) -> Dict[str, Any]:
        """Generate relevant business objectives for the domain"""
        objectives = {
            "fintech": {
                "primary": "Increase user trust and platform adoption",
                "secondary": "Ensure regulatory compliance and risk management",
                "metrics": ["User acquisition", "Asset under management", "Compliance score"]
            },
            "healthcare": {
                "primary": "Improve patient outcomes and provider efficiency",
                "secondary": "Ensure data security and regulatory compliance",
                "metrics": ["Patient satisfaction", "Time to treatment", "Compliance rate"]
            },
            "ecommerce": {
                "primary": "Increase conversion rate and customer lifetime value",
                "secondary": "Reduce cart abandonment and support costs",
                "metrics": ["Conversion rate", "Average order value", "Customer retention"]
            }
        }
        
        return objectives.get(domain, {
            "primary": "Achieve operational efficiency and user satisfaction",
            "secondary": "Ensure scalability and maintainability",
            "metrics": ["User engagement", "System performance", "Cost efficiency"]
        })
    
    def _generate_technical_requirements(self, domain: str) -> Dict[str, Any]:
        """Generate relevant technical requirements for the domain"""
        requirements = {
            "fintech": {
                "performance": ["Sub-second response time", "99.99% uptime", "Global scale"],
                "security": ["End-to-end encryption", "Multi-factor authentication", "Audit logging"],
                "integration": ["Banking APIs", "Compliance systems", "Analytics platforms"]
            },
            "healthcare": {
                "performance": ["Real-time data access", "High availability", "Disaster recovery"],
                "security": ["HIPAA compliance", "Data encryption", "Access controls"],
                "integration": ["EHR systems", "Lab systems", "Billing platforms"]
            },
            "ecommerce": {
                "performance": ["Fast page load", "Mobile optimization", "Peak traffic handling"],
                "security": ["PCI compliance", "Fraud detection", "Secure payments"],
                "integration": ["Payment gateways", "Inventory systems", "Shipping providers"]
            }
        }
        
        return requirements.get(domain, {
            "performance": ["Fast response time", "High availability", "Scalable architecture"],
            "security": ["Data protection", "Access control", "Audit trails"],
            "integration": ["Third-party APIs", "Legacy systems", "Analytics tools"]
        })
    
    def _generate_regulatory_context(self, domain: str) -> Dict[str, Any]:
        """Generate relevant regulatory context for the domain"""
        if domain in self.context_templates and "regulatory_context" in self.context_templates[domain]:
            return self.context_templates[domain]["regulatory_context"]
        
        return {
            "compliance_requirements": ["Data protection", "Accessibility", "Security standards"],
            "industry_standards": ["ISO standards", "Security frameworks", "Best practices"],
            "audit_requirements": ["Regular assessments", "Documentation", "Remediation"]
        }
    
    def _generate_competitive_context(self, domain: str) -> Dict[str, Any]:
        """Generate relevant competitive context for the domain"""
        return {
            "differentiation_opportunities": ["User experience", "Security", "Performance", "Integration"],
            "market_trends": ["Industry evolution", "User expectations", "Technology advances"],
            "competitive_advantages": ["Speed to market", "Technical innovation", "User trust"]
        }
    
    def _enhance_specificity(self, input_line: str, analysis: Dict[str, Any], context: Dict[str, Any]) -> str:
        """Enhance the specificity of the prompt"""
        
        domain = analysis["domain"]
        prompt_type = analysis["prompt_type"]
        complexity = analysis["complexity_level"]
        primary_goal = analysis["primary_goal"]
        
        # Start with enhanced opening
        enhanced_prompt = f"Design a {complexity} {domain} solution that {primary_goal}"
        
        # Add specific context
        if "user_personas" in context:
            enhanced_prompt += f" for {context['user_personas']['primary']}"
        
        # Add business context
        if "business_objectives" in context:
            enhanced_prompt += f", achieving {context['business_objectives']['primary']}"
        
        # Add technical requirements
        if "technical_requirements" in context:
            tech_reqs = context['technical_requirements']
            if 'performance' in tech_reqs:
                enhanced_prompt += f" with {', '.join(tech_reqs['performance'][:2])}"
        
        # Add regulatory context
        if "regulatory_compliance" in context:
            reg_context = context['regulatory_compliance']
            if 'compliance_requirements' in reg_context:
                enhanced_prompt += f" while ensuring {', '.join(reg_context['compliance_requirements'][:2])}"
        
        # Add success criteria
        if "business_objectives" in context and "metrics" in context["business_objectives"]:
            metrics = context["business_objectives"]["metrics"]
            enhanced_prompt += f". Success measured by {', '.join(metrics)}"
        
        return enhanced_prompt
    
    def _add_actionability(self, enhanced_prompt: str, analysis: Dict[str, Any], context: Dict[str, Any]) -> str:
        """Add actionability to the enhanced prompt"""
        
        actionable_prompt = enhanced_prompt + "\n\n"
        
        # Add specific deliverables
        actionable_prompt += "**Deliverables:**\n"
        actionable_prompt += "1. User experience wireframes with interaction flows\n"
        actionable_prompt += "2. Technical architecture diagram with security considerations\n"
        actionable_prompt += "3. Implementation roadmap with milestones and success criteria\n"
        actionable_prompt += "4. Risk assessment and mitigation strategies\n"
        actionable_prompt += "5. Testing and validation framework\n\n"
        
        # Add constraints and considerations
        actionable_prompt += "**Key Considerations:**\n"
        
        if "regulatory_compliance" in context:
            actionable_prompt += "- Regulatory compliance and audit requirements\n"
        
        if "technical_requirements" in context:
            actionable_prompt += "- Technical performance and security standards\n"
        
        if "user_personas" in context:
            actionable_prompt += "- User experience and accessibility requirements\n"
        
        if "competitive_landscape" in context:
            actionable_prompt += "- Competitive differentiation and market positioning\n"
        
        actionable_prompt += "- Scalability and future-proofing considerations\n\n"
        
        # Add success criteria
        actionable_prompt += "**Success Criteria:**\n"
        if "business_objectives" in context and "metrics" in context["business_objectives"]:
            for metric in context["business_objectives"]["metrics"]:
                actionable_prompt += f"- {metric} improvement with measurable targets\n"
        
        actionable_prompt += "- User satisfaction and adoption metrics\n"
        actionable_prompt += "- Technical performance and reliability benchmarks\n"
        actionable_prompt += "- Compliance and security validation results\n"
        
        return actionable_prompt
    
    def _validate_prompt_quality(self, prompt: str) -> Dict[str, Any]:
        """Validate the quality of the enhanced prompt"""
        
        # Calculate various quality scores
        specificity_score = min(1.0, len(prompt.split()) / 100)  # Based on detail level
        context_score = min(1.0, prompt.count("**") / 10)  # Based on structured sections
        actionability_score = min(1.0, prompt.count("Deliverables") + prompt.count("Success Criteria"))
        completeness_score = min(1.0, (prompt.count("user") + prompt.count("technical") + prompt.count("business")) / 10)
        
        overall_score = (specificity_score + context_score + actionability_score + completeness_score) / 4
        
        return {
            "specificity_score": specificity_score,
            "context_score": context_score,
            "actionability_score": actionability_score,
            "completeness_score": completeness_score,
            "overall_score": overall_score,
            "word_count": len(prompt.split()),
            "structure_elements": prompt.count("**"),
            "improvement_ratio": overall_score / 0.2  # Assuming baseline of 0.2
        }

def demonstrate_super_prompt_engineer():
    """Demonstrate the Super Prompt Engineer with examples"""
    
    print("üöÄ SUPER PROMPT ENGINEER DEMONSTRATION")
    print("=" * 60)
    
    engineer = SuperPromptEngineer()
    
    # Test cases with single lines
    test_cases = [
        "Design crypto trading app authentication",
        "Create user onboarding flow",
        "Build healthcare patient portal",
        "Develop AI-powered analytics dashboard",
        "Design secure payment system"
    ]
    
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nüîç TEST CASE {i}")
        print("=" * 30)
        
        result = engineer.engineer_super_prompt(test_case)
        results.append(result)
        
        print(f"üìä ENHANCEMENT METRICS:")
        metrics = result["enhancement_metrics"]
        print(f"   Specificity: {metrics['specificity_improvement']:.2f}")
        print(f"   Context Richness: {metrics['context_richness']:.2f}")
        print(f"   Actionability: {metrics['actionability']:.2f}")
        print(f"   Overall Quality: {metrics['overall_quality']:.2f}")
        
        print(f"\nüìù ENHANCED PROMPT (Preview):")
        preview = result["enhanced_prompt"][:200] + "..." if len(result["enhanced_prompt"]) > 200 else result["enhanced_prompt"]
        print(f"   {preview}")
        
        print(f"\nüìà IMPROVEMENT:")
        validation = result["validation"]
        print(f"   Word Count: {validation['word_count']} (vs ~5 baseline)")
        print(f"   Structure Elements: {validation['structure_elements']}")
        print(f"   Improvement Ratio: {validation['improvement_ratio']:.1f}x")
    
    # Summary
    print(f"\nüèÜ SUPER PROMPT ENGINEER SUMMARY")
    print("=" * 40)
    
    avg_specificity = sum(r["enhancement_metrics"]["specificity_improvement"] for r in results) / len(results)
    avg_context = sum(r["enhancement_metrics"]["context_richness"] for r in results) / len(results)
    avg_actionability = sum(r["enhancement_metrics"]["actionability"] for r in results) / len(results)
    avg_overall = sum(r["enhancement_metrics"]["overall_quality"] for r in results) / len(results)
    avg_improvement = sum(r["validation"]["improvement_ratio"] for r in results) / len(results)
    
    print(f"Average Specificity Score: {avg_specificity:.2f}")
    print(f"Average Context Richness: {avg_context:.2f}")
    print(f"Average Actionability: {avg_actionability:.2f}")
    print(f"Average Overall Quality: {avg_overall:.2f}")
    print(f"Average Improvement Ratio: {avg_improvement:.1f}x")
    
    print(f"\n‚úÖ Super Prompt Engineer transforms single lines into comprehensive,")
    print(f"   context-rich, actionable prompts with {avg_improvement:.1f}x improvement!")
    
    return results

if __name__ == "__main__":
    results = demonstrate_super_prompt_engineer()
    
    print(f"\nüìÑ Results saved to super_prompt_results.json")
    with open('super_prompt_results.json', 'w') as f:
        json.dump(results, f, indent=2) 